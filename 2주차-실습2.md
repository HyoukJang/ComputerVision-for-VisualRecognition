# MNIST 데이터셋을 통한 SVM 학습
* 

본 파일 실행을 위해 [data2의 MNIST.zip](https://github.com/moduPlayGround/ComputerVision-for-VisualRecognition/blob/master/2%EC%A3%BC%EC%B0%A8-%EC%8B%A4%EC%8A%B5/data2/MNIST.zip)의 압축을 풀어야 합니다.
<br>


## MNIST 데이터 HOG(Histogram of Gradient)로 특징량 검출

### HOG : [참고가 될 설명](https://donghwa-kim.github.io/hog.html)


### Input Iamge Information : 20x20의 MNIST Image 5000장 (각 클래스(0~9) 별 500장)

#### 가장먼저 HOG Descriptor를 추출할 이미지를 OPenCV의 `imread`를 이용해 불러옵니다. 이번 실습에서 제공하는 MNIST Image의 이름 정의에 대해서 'data 4_122'를 예로들어 간단히 설명하자면,'data 4_122'라는 이름을 갖는 이미지는 4를 나타내는 손글씨 사진 중 122번째를 의미합니다. 0~9까지의 숫자를 손글씨로 작성하였고 각 클래스별로 총 500장이 있습니다.

```
	cout << "\n//////////////////////////////////////////////////////" << endl;

	cout << "train Image load" << endl;

	for(int num=0; num<10; num++)
	{
		cout << num << " image load" << endl;
		for (int i = 0; i < NUMBER_train; i++)
		{
			Mat MNIST = imread("./MNIST/data "+ std::to_string(num) + "_" +std::to_string(i + 1) + ".PNG",COLOR_RGB2GRAY);
			if (!(MNIST.data))
			{
				cout << "image load fail" << endl;
				return 0;
			}
			//imshow("train", MNIST);
			//waitKey(1);
			MNIST_train_HOG.push_back(find_HOG_feature_image(MNIST));
			MNIST_train_label.push_back(num);
		}

	}
	cout << "\n//////////////////////////////////////////////////////" << endl;

```

#### 다음으로는 OPencv에서 제공하는 HOGDescriptor class를 통해서 HOGDescriptor를 계산해야합니다. 이 과정을 진행하는 함수는 다음과 같습니다.

```
vector<float> find_HOG_feature_image(Mat img)
{
	HOGDescriptor IMAGE_HOG
	(
		Size(20, 20), //winSize
		Size(8, 8), //blocksize
		Size(4, 4), //blockStride,
		Size(8, 8), //cellSize,
		9, //nbins,
    /////////////// 밑에 파라미터는 추가적으로 공부하시길 바랍니다.
		1, //derivAper,
		-1, //winSigma,
		0, //histogramNormType,
		0.2, //L2HysThresh,
		0,//gammal correction,
		64//nlevels=64
	);

	vector<float> hog_descriptor;
	IMAGE_HOG.compute(img, hog_descriptor);
	return hog_descriptor;

}
```
![ezgif com-gif-maker](https://user-images.githubusercontent.com/44772344/52755939-0c5eeb00-3043-11e9-9e1c-c9a236566788.gif) <br>

#### HOGDescriptor를 사용하기위해 `HOGDescriptor IMAGE_HOG` 와 같이 설정합니다. 다만 이 과정에서 자신이 입력할 이미지에 따라 Block size, Block Stride, cell size를 설정해주고 총 몇개의 회전방향을 구분할지 nbins를 설정해주면 됩니다. 이후의 변수들은 일반적으로는 디폴트 값을 설정합니다. 이 과정을 통해서 입력한 MNIST 이미지 5000장에 대한 `HOG Decriptor`를 추출해 저장합니다. 이 때  `HOG Decriptor`는 `vector<vector<float>>`의 형태로 저장되고 있습니다. 이후에는 SVM(Support Vector Machine)을 통해서 분류를 진행할 예정인데, OPENCV에서 주어지는 SVM함수는 다음과 같이 Mat 형태를 argument로 요구하고 있습니다. 따라서 `ConvertVectortoMatrix` 함수를 통해서 `vector<vector<float>>`형태로 저장한 `HOG Decriptor`를 Mat 형태로 변환해줍니다. 

```
	int descriptor_size = MNIST_train_HOG[0].size();
	Mat MNIST_train_HOG_Mat(MNIST_train_HOG.size(), descriptor_size, CV_32FC1);
	Mat MNIST_test_HOG_Mat(MNIST_test_HOG.size(), descriptor_size, CV_32FC1);
	ConvertVectortoMatrix(MNIST_train_HOG, MNIST_test_HOG, MNIST_train_HOG_Mat, MNIST_test_HOG_Mat);
	
	....
	
	void ConvertVectortoMatrix(vector<vector<float> > &trainHOG, vector<vector<float> > &testHOG, Mat &trainMat, Mat &testMat)
{

	int descriptor_size = trainHOG[0].size();

	for (int i = 0; i < trainHOG.size(); i++)
	{
		for (int j = 0; j < descriptor_size; j++) 
		{
			trainMat.at<float>(i, j) = trainHOG[i][j];
		}
	}
	for (int i = 0; i < testHOG.size(); i++)
	{
		for (int j = 0; j < descriptor_size; j++)
		{
			testMat.at<float>(i, j) = testHOG[i][j];
		}
	}
}

```
#### 다음과 같이 vector에서 Mat 형태로 변환했다면 이제 SVM을 이용해서 분류를 진행합니다. SVM을 이용해서 분류를 진행하기 위해서는 파라미터 설정과 초기 학습과정이 필요합니다. 해당 부분은 다음과 같습니다. 

```


	CvSVMParams params;
	params.svm_type = CvSVM::C_SVC;
	params.kernel_type = CvSVM::RBF;
	params.gamma = 0.50625;
	params.C = 2.5;
	CvSVM svm;


	CvMat tryMat = MNIST_train_HOG_Mat;
	Mat trainLabelsMat(MNIST_train_label.size(), 1, CV_32FC1);

	for (int i = 0; i < MNIST_train_label.size(); i++)
	{
		trainLabelsMat.at<float>(i, 0) = MNIST_train_label[i];
	}


	CvMat tryMat_2 = trainLabelsMat;
	svm.train(&tryMat, &tryMat_2, Mat(), Mat(), params);
```

#### 해당 과정에 대해서 설명하자면, 해당 입력으로 들어가는 파라미터에 대한 값은 실험적으로 가장 높은 정확도를 보이는 파라미터를 구해야합니다. 이에 대한 과정은 해당 실습에서는 생략하겠습니다. 해당 코드에서는 실험을 통해서 구한 파라미터인 `params.gamma = 0.50625`와 `params.C = 2.5`를 입력했습니다. 이러한 파라미터를 갖는 SVM을 생성한 후 학습을 위해서 데이터를 입력합니다. SVM을 학습하기위해서는 (OPencv2.4버전기준) 위에서  구한`HOG Decriptor`를 포함하는 Mat 과 각 `HOG Decriptor`가 가르키는 Label 값을 정리한 Mat을 입력해줘야 합니다. 좀 더 쉽게 설명하자면 다음과 같습니다. 

![svm_input](https://user-images.githubusercontent.com/44772344/52757499-2f3fce00-3048-11e9-8af6-a5f6d723ea6a.png)

#### 다음과 같은 데이터를 `svm.train`에 입력하면 분류기는 자동으로 입력된 데이터들을 라벨에 따라서 구분짓고 이후에 데이터가 들어왔을때 들어온 데이터가 어떠한 영역에 해당되는지에 대한 예측을 진행할 수 있습니다. 

#### 학습을 마쳤다면 이제 예측을 진행해보겠습니다. 위에서 train 이미지를 불러온 방법과 마찬가지로 이전에 불러온적 없는 test 이미지를 불러오고 각 이미지에 대한 descriptor를 구합니다. 그리고 이 구한 descriptor를 svm에 넣어서 어떠한 결과가 나오는지 정확도를 구합니다. 이때 정확도는 test 데이터에 대한 라벨 정보를 갖고 있기 때문에 Svm으로 분류된 값이랑 기존 label 값이랑 비교해 구할 수 있습니다. 해당 파트에 대한 부분은 다음에서 확인할 수 있습니다.

```
svm.predict(MNIST_test_HOG_Mat, testResponse);

	float count = 0, accuracy = 0;
	for (int i = 0; i < testResponse.rows; i++)
	{
		if (testResponse.at<float>(i, 0) == MNIST_test_label[i])
		{
			count = count + 1;
		}
	}

	accuracy = (count / testResponse.rows) * 100;


	cout << "accuracy : " << accuracy << endl;
```

#### 해당 과정을 통한 이번 이미지에 대한 정확도는 약 97%가 나오는것을 확인할 수 있습니다. 추가적으로 매번 train 이미지의 descriptor를 구하고 svm을 train하는 과정은 이번 실습에서는 5000장이였지만, 이미지가 많아지면 많은 시간이 소모됩니다. 따라서 학습한 svm을 다음과 같이 저장해서 다음에는 바로 svm.predict을 진행할 수 있습니다.

```
	/// Save svm file
	svm.save("MNIST_HOG_SVM.xml");
	
	/// load svm file for predict

	svm.predict(MNIST_test_HOG_Mat, testResponse);

	float count = 0, accuracy = 0;
	for (int i = 0; i < testResponse.rows; i++)
	{
		if (testResponse.at<float>(i, 0) == MNIST_test_label[i])
		{
			count = count + 1;
		}
	}

	accuracy = (count / testResponse.rows) * 100;


	cout << "accuracy : " << accuracy << endl;

```
####  이역시 높은 정확도를 보이고 있습니다. 
